/**********************************************************
 * C-based/Cached/Core Computer Vision Library
 * Liu Liu, 2010-02-01
 **********************************************************/

/**********************************************************
 * CCV - Neural Network Collection
 **********************************************************/

#ifndef GUARD_ccv_nnc_h
#define GUARD_ccv_nnc_h

#include <ccv.h>

// These are generated by cmd/build-cmd.rb
#include "cmd/ccv_nnc_cmd.h"
#include "cmd/ccv_nnc_backend.h"

enum {
	// Attributes that enable symbolic graph simplification
	CCV_NNC_CMD_ATTR_PASSTHROUGH  = 0x01, // This doesn't compute anything, but pass the first n tensors to the output (useful for backprop that is identical).
	CCV_NNC_CMD_ATTR_OUTPUT_ONES  = 0x02, // All the output tensors are 1s (unit).
	CCV_NNC_CMD_ATTR_NULL_IS_ONES = 0x04, // Accept nullptr input as if these are tensors with 1s (unit).
};

// Flags pass into cmd when executing.
enum {
	CCV_NNC_ACCUMULATE_OUTPUT = 0x01, // Enable accumulate outputs.
	CCV_NNC_ZERO_MEMORY_ALLOC = 0x02, // Don't allocate any extra memory for this operation.
};

enum {
	CCV_NNC_EXEC_SUCCESS   = 0,
	CCV_NNC_EXEC_INVALID   = -1, // Invalid input.
	CCV_NNC_EXEC_NO_KERNEL = -2,
	CCV_NNC_EXEC_OOM       = -3,
};

typedef struct {
	struct {
		int dim[CCV_NNC_MAX_DIM_ALLOC];
	} size; /**< [size] The window size for the layer. For full connect layer, it is 1 because it is 1x1 convolutional layer with count of filters */
	union {
		struct {
			int count; /**< [convolution.count] The number of filters for convolutional layer. */
			int groups; /**< [convolution.groups] The number of groups for convolutional layer. */
		} convolution;
		struct {
			int reserved;
		} pool;
		struct {
			float kappa; /**< [rnorm.kappa] As of b[i] = a[i] / (rnorm.kappa + rnorm.alpha * sum(a, i - rnorm.size / 2, i + rnorm.size / 2)) ^ rnorm.beta */
			float alpha; /**< [rnorm.alpha] See **rnorm.kappa**. */
			float beta; /**< [rnorm.beta] See **rnorm.kappa**. */
		} rnorm;
		struct {
			int axis[CCV_NNC_MAX_DIM_ALLOC]; /**< [bnorm.axis[]] The axis selected to compute mean / variance. */
			int count; /**< [bnorm.count] The number of axis selected. */
			float epsilon; /**< [bnorm.epsilon] The epsilon for standard derivation. */
			int is_test; /**< [bnorm.is_test] Whether in test mode. */
			float momentum; /**< [bnorm.momentum] running_mean = running_mean * momentum + mean * (1 - momentum). */
		} bnorm;
		struct {
			float a[3]; /**< [blas.a[3]] BLAS scalars. */
			int count; /**< [blas.count] The number of outputs for blas layer. */
		} blas;
		struct {
			int axis[CCV_NNC_MAX_DIM_ALLOC]; /**< [reduce.axis[]] The axis selected to reduce. */
			int count; /**< [reduce.count] The number of axis selected. */
		} reduce;
		struct {
			float p; /**< [dropout.p] Dropout probability. */
		} dropout;
		void* userdata;
	};
} ccv_nnc_cmd_param_t;

typedef struct {
	struct {
		int dim[CCV_NNC_MAX_DIM_ALLOC];
	} stride;
	struct {
		int begin[CCV_NNC_MAX_DIM_ALLOC];
		int end[CCV_NNC_MAX_DIM_ALLOC];
	} border;
} ccv_nnc_hint_t;

typedef struct ccv_nnc_stream_context_s ccv_nnc_stream_context_t;

typedef struct ccv_nnc_cmd_s {
	uint32_t cmd;
	uint32_t backend;
	int algorithm;
	ccv_nnc_cmd_param_t info;
	// This has to be the same as the ccv_nnc_cmd_exec_f type.
	// This is for type CCV_NNC_COMPUTE_CUSTOM
	int(*exec)(const struct ccv_nnc_cmd_s cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const ccv_nnc_stream_context_t* stream_context);
} ccv_nnc_cmd_t;

// For forward functions, the input tensors and output tensors can be arbitrary.
// However, for backward functions (backpropagation, or gradient functions in other libs),
// the input is: 0~m-1: gradient for output tensors, 1~n: input tensors for forward functions, n+1~n+m: output tensors for forward functions,
// the output is: 0~n-1: output gradients w.r.t. input tensors.
// Which input / output tensors can be ignored can be specified in the cmd config structs.
typedef int(*ccv_nnc_cmd_exec_f)(const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const ccv_nnc_stream_context_t* stream_context);

typedef int(*ccv_nnc_cmd_autotune_f)(const ccv_nnc_cmd_t cmd, const size_t max_workspace_size, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const ccv_nnc_stream_context_t* stream_context);

static inline int ccv_nnc_tensor_nd(const int dim[CCV_NNC_MAX_DIM_ALLOC])
{
	int i;
	for (i = 0; i < CCV_NNC_MAX_DIM_ALLOC; i++)
		if (dim[i] == 0)
			return i;
	return CCV_NNC_MAX_DIM_ALLOC;
}

/**
 * Level-0 API
 */

void ccv_nnc_init(void);

/**
 * Level-1 API
 */

// For tensor
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_new(const void* const ptr, const ccv_nnc_tensor_param_t params, const int flags);
// Allocating on stack
CCV_WARN_UNUSED(ccv_nnc_tensor_t) ccv_nnc_tensor(const void* const ptr, const ccv_nnc_tensor_param_t params, const int flags);
void ccv_nnc_tensor_free(ccv_nnc_tensor_t* const tensor);
CCV_WARN_UNUSED(ccv_nnc_tensor_view_t*) ccv_nnc_tensor_view_new(const ccv_nnc_tensor_t* const tensor, const int dim[CCV_NNC_MAX_DIM_ALLOC], const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC]);
// Allocating on stack
CCV_WARN_UNUSED(ccv_nnc_tensor_view_t) ccv_nnc_tensor_view(const ccv_nnc_tensor_t* const tensor, const int dim[CCV_NNC_MAX_DIM_ALLOC], const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC]);
void ccv_nnc_tensor_view_free(ccv_nnc_tensor_view_t* const tensor_view);
// All these functions afterwards should be compatible with both tensor and tensor view unless assertion.
void ccv_nnc_tensor_zero(void* const tensor);
CCV_WARN_UNUSED(int) ccv_nnc_tensor_eq(const ccv_nnc_tensor_t* const a, const ccv_nnc_tensor_t* const b);

// For computation node
// Return high precision time unit.
uint64_t ccv_nnc_cmd_mono_time(void);
CCV_WARN_UNUSED(const char*) ccv_nnc_cmd_name(const uint32_t cmd);
CCV_WARN_UNUSED(const char*) ccv_nnc_cmd_backend_name(const uint32_t backend);
CCV_WARN_UNUSED(int) ccv_nnc_cmd_ok(const uint32_t cmd, const uint32_t backend);
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_cmd(const uint32_t cmd, ccv_nnc_cmd_exec_f exec, const ccv_nnc_cmd_param_t params, const int flags);
// Verify the hint
CCV_WARN_UNUSED(int) ccv_nnc_hint_verify(const ccv_nnc_hint_t hint, const ccv_nnc_cmd_param_t cmd, const ccv_nnc_tensor_param_t a, const ccv_nnc_tensor_param_t b);
// Auto find the best hint for a given input / output (on forward pass only).
CCV_WARN_UNUSED(ccv_nnc_hint_t) ccv_nnc_hint_auto(const ccv_nnc_cmd_param_t cmd, const ccv_nnc_tensor_param_t a, const ccv_nnc_tensor_param_t b);
// Auto find the outputs for the given inputs / hint.
void ccv_nnc_hint_tensor_auto(const ccv_nnc_cmd_t cmd, const ccv_nnc_tensor_param_t* const inputs, const int input_size, const ccv_nnc_hint_t hint, ccv_nnc_tensor_param_t* const outputs, const int output_size);
CCV_WARN_UNUSED(uint32_t) ccv_nnc_cmd_find_backend(const ccv_nnc_cmd_t cmd, const int tensor_memory, const int tensor_formats, const int tensor_datatypes);
// Run autotune to find the best kernel and configuration for the given input, returned is the modified
// cmd that contains the updated configuration.
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_cmd_autotune(const ccv_nnc_cmd_t cmd, const size_t max_workspace_size, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const ccv_nnc_stream_context_t* const stream_context);
CCV_WARN_UNUSED(int) ccv_nnc_cmd_bitmask(const ccv_nnc_cmd_t cmd, const int input_size, const int output_size, const uint64_t* const input_bitmasks, const int input_bitmask_size, const uint64_t* const output_bitmasks, const int output_bitmask_size);
int ccv_nnc_cmd_exec(const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const ccv_nnc_stream_context_t* const stream_context);
CCV_WARN_UNUSED(int) ccv_nnc_cmd_is_forward(const ccv_nnc_cmd_t cmd);
CCV_WARN_UNUSED(int) ccv_nnc_cmd_is_backward(const ccv_nnc_cmd_t cmd);
// Check this command against listed attributes.
CCV_WARN_UNUSED(int) ccv_nnc_cmd_attr(const ccv_nnc_cmd_t cmd, const int flags);
// Check whether this command allow inplace operation against a particular input and output (index from 0).
CCV_WARN_UNUSED(int) ccv_nnc_cmd_allow_inplace(const ccv_nnc_cmd_t cmd, const int input_idx, const int output_idx);
// Check whether this command need to enforce inplace operation against a particular input and output (index from 0).
CCV_WARN_UNUSED(int) ccv_nnc_cmd_enforce_inplace(const ccv_nnc_cmd_t cmd, const int input_idx, const int output_idx);

// Control flow constructs
// Follow heavily based along CUDA's stream / event idea.
enum {
	CCV_STREAM_CONTEXT_CPU = 0x1,
	CCV_STREAM_CONTEXT_GPU = 0x2,
};
#define CCV_STREAM_GET_CONTEXT(type) ((type) & 0x3)
#define CCV_STREAM_GET_DEVICE(type) ((type) & 0xff00)
#define CCV_STREAM_GET_DEVICE_ID(type) (CCV_STREAM_GET_DEVICE(type) >> 8)
// Flag is a combination of CPU / GPU and DEVICE_ID
CCV_WARN_UNUSED(ccv_nnc_stream_context_t*) ccv_nnc_stream_context_new(const int type);
void ccv_nnc_stream_context_wait(const ccv_nnc_stream_context_t* const stream);
void ccv_nnc_stream_context_free(ccv_nnc_stream_context_t* const stream_context);

typedef struct ccv_nnc_stream_signal_s ccv_nnc_stream_signal_t;

CCV_WARN_UNUSED(ccv_nnc_stream_signal_t*) ccv_nnc_stream_signal_new(const int type);
void ccv_nnc_stream_context_emit_signal(const ccv_nnc_stream_context_t* const stream, const ccv_nnc_stream_signal_t* const signal);
void ccv_nnc_stream_context_wait_signal(const ccv_nnc_stream_context_t* const stream, const ccv_nnc_stream_signal_t* const signal);
void ccv_nnc_stream_signal_free(ccv_nnc_stream_signal_t* const signal);

/**
 * Level-2 API
 */

enum {
	CCV_NNC_SHORT_DOT_GRAPH = 0x0,
	CCV_NNC_LONG_DOT_GRAPH  = 0x1,
};

typedef struct ccv_nnc_graph_s ccv_nnc_graph_t;

typedef struct {
	int32_t d; // This is int because sometimes I piggy-back on negatives to carry out some internal computations.
	const ccv_nnc_graph_t* graph;
} ccv_nnc_graph_exec_t;

#define CCV_NO_GRAPH_EXEC(exec) ((exec).graph == 0)

// Create an empty graph.
// Note that all graph mutation methods are not thread-safe.
// You should only operate the graph in serial fashion.
CCV_WARN_UNUSED(ccv_nnc_graph_t*) ccv_nnc_graph_new(void);
// Create a node with specific command execution, as well as its inputs & outputs.
// Underlying, the graph maintains the backing object for the node, and all you get is
// a on-stack object to index the backing object from the graph.
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_new(ccv_nnc_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size);
void ccv_nnc_graph_exec_set_hint(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const ccv_nnc_hint_t hint);
void ccv_nnc_graph_exec_set_io(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size);
// This must be called after set_io, set additional flags for tensors related to this exec.
void ccv_nnc_graph_exec_set_io_flags(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const int* const input_flags, const int input_flag_size, const int* const output_flags, const int output_flag_size);
// Set the peer reference for exec.
void ccv_nnc_graph_exec_set_peer(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const ccv_nnc_graph_exec_t peer_exec);
// Updates are the tensors that not directly involved in the computation, but its pointers need to get updated along with this exec, thus need to be "update" to other exec nodes.
void ccv_nnc_graph_exec_add_update(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_tensor_t* const update);
// Concatenate input graph nodes with an output graph node to create a new graph.
// Return non-zero if cannot concat successfully.
int ccv_nnc_graph_exec_concat(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t source, const ccv_nnc_graph_exec_t destination);
// Disconnect input graph nodes with an output graph nodes in this graph.
// Return non-zero if cannot disjoin successfully.
int ccv_nnc_graph_exec_disjoin(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t source, const ccv_nnc_graph_exec_t destination);
// Number of exec in the graph.
int ccv_nnc_graph_exec_size(const ccv_nnc_graph_t* const graph);
// Generate output that can be parsed by GraphViz (DOT language).
void ccv_nnc_graph_dot(const ccv_nnc_graph_t* const graph, const int flags, FILE* out);
// Run the autotune function for all the inputs / outputs, afterwards, assigning the optimized cmd back.
void ccv_nnc_graph_autotune(ccv_nnc_graph_t* const graph, const size_t max_workspace_size, const int flags, const ccv_nnc_graph_exec_t* const sources, const int source_size, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);
// Make the graph sequential, thus, do a topological sort so when run the graph, no additional memory will be allocated.
void ccv_nnc_graph_sequential(ccv_nnc_graph_t* const graph, int* const exec_cvt, const int exec_cvt_size);
// The sources / destinations.
void ccv_nnc_graph_set_sources(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t* const sources, const int source_size);
ccv_nnc_graph_exec_t* ccv_nnc_graph_sources(const ccv_nnc_graph_t* const graph);
int ccv_nnc_graph_source_size(const ccv_nnc_graph_t* const graph);
void ccv_nnc_graph_set_destinations(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);
ccv_nnc_graph_exec_t* ccv_nnc_graph_destinations(const ccv_nnc_graph_t* const graph);
int ccv_nnc_graph_destination_size(const ccv_nnc_graph_t* const graph);
// add tensor pair that can be used to "carry over". (carry over: passing a tensor from current loop to the next loop).
void ccv_nnc_graph_add_carry_over(ccv_nnc_graph_t* const graph, const ccv_nnc_tensor_t* const from, const ccv_nnc_tensor_t* const to);
// This graph, and its relevant auxiliary objects (opaque to user) are deallocated.
void ccv_nnc_graph_free(ccv_nnc_graph_t* const graph);

/**
 * Level-3 API
 */

typedef struct ccv_nnc_symbolic_graph_s ccv_nnc_symbolic_graph_t;

// Opaque pointer to an arena of allocated tensors.
typedef struct ccv_nnc_tensor_arena_s ccv_nnc_tensor_arena_t;

// Opaque pointer to an arena of allocated execs.
typedef struct ccv_nnc_graph_exec_arena_s ccv_nnc_graph_exec_arena_t;

typedef struct {
	ccv_nnc_tensor_param_t info;
	int32_t d;
	const ccv_nnc_symbolic_graph_t* graph;
} ccv_nnc_tensor_symbol_t;

typedef struct {
	int32_t d;
	const ccv_nnc_symbolic_graph_t* graph;
} ccv_nnc_graph_exec_symbol_t;

enum {
	CCV_NNC_TENSOR_SYMBOL_INIT_ZEROS = 0x01, // Initialize underlying tensor for the symbol with zeros
	CCV_NNC_TENSOR_SYMBOL_TAPE_VAR = 0x02, // Mark this as a tape variable (it cannot be folded, will contain flag CCV_TAPE_ALLOC)
	// The one below is special.
	CCV_NNC_TENSOR_SYMBOL_DEAD = 0x80000000, // Mark this tensor symbol as dead, any future usage will cause assertion
};

enum {
	CCV_NNC_GRAPH_EXEC_DEAD = 0x1, // Mark this node as dead.
	CCV_NNC_GRAPH_EXEC_P_WHILE = 0x10, // Mark this node keyword is while
	CCV_NNC_GRAPH_EXEC_CASE_OF = 0x20, // Mark this node keyword is case_of
};

#define CCV_NNC_GRAPH_EXEC_IS_DEAD(x) ((x) & CCV_NNC_GRAPH_EXEC_DEAD)
#define CCV_NNC_GRAPH_REF(x) ((x)->_heap_graph_ref ? (x)->_heap_graph_ref : (x)->_inline_graph_ref)

enum {
	CCV_NNC_NO_TENSOR_SYMBOL = -1,
	CCV_NNC_WHILE_COUNT_TENSOR_SYMBOL = -2,
};

#define CCV_NNC_IS_WHILE_COUNT_TENSOR_SYMBOL(d) (((uint32_t)(d) & 0xf) == 0xe)

typedef struct {
	ccv_nnc_tensor_symbol_t source;
	ccv_nnc_tensor_symbol_t destination;
} ccv_nnc_tensor_symbol_map_t;

// Create an empty symbolic graph.
// Note that all graph mutation methods are not thread-safe.
// You should only operate the graph in serial fashion.

// Create a new symbolic graph. It is an opaque data structure that maintains the whole graph of computation in its symbolic form.
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_new(void);
// Create an tensor symbol (thus, with no actual memory space allocation) in a symbolic graph.
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_param_t info, const char* const name);
// Create an alias to the tensor symbol as tensor view (thus, pointing to the same memory region, but with a different header info and offset).
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_alias_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol, const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC], const ccv_nnc_tensor_param_t info, const char* const name);
// For a given tensor symbol, this method resolve to its local reference inside the given graph.
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_resolve(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol);
// Set the peer reference for tensor.
void ccv_nnc_tensor_symbol_set_peer(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor_symbol, const ccv_nnc_tensor_symbol_t peer_tensor_symbol);
// Pass graph's tensor symbol into its sub graph.
void ccv_nnc_tensor_symbol_hookup(ccv_nnc_symbolic_graph_t* const src_graph, ccv_nnc_symbolic_graph_t* const dest_graph, const ccv_nnc_tensor_symbol_t src_tensor_symbol, const ccv_nnc_tensor_symbol_t dest_tensor_symbol);
// Set bypasses for a tensor symbol.
void ccv_nnc_tensor_symbol_set_bypasses(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);
// Create a graph node (an operation that takes a set of inputs and generates a set of outputs).
ccv_nnc_graph_exec_symbol_t ccv_nnc_graph_exec_symbol_new(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const char* const name);
// Return the command on this exec symbol
CCV_WARN_UNUSED(ccv_nnc_cmd_t) ccv_nnc_graph_exec_symbol_cmd(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec);
// The operation defaults to use `ccv_nnc_hint_auto` find the best hints for a set of inputs / outputs.
// However, you can also set your own hints. Return non-zero if cannot set successfully.
int ccv_nnc_graph_exec_symbol_set_hint(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, const ccv_nnc_hint_t hint);
void ccv_nnc_graph_exec_symbol_set_io(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size);
// Set the peer reference for exec.
void ccv_nnc_graph_exec_symbol_set_peer(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec_symbol, const ccv_nnc_graph_exec_symbol_t peer_exec_symbol);
// Set the tensor symbol info again. Thus, its dimensionality depends on the tensor input.
int ccv_nnc_tensor_symbol_set(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor, const ccv_nnc_tensor_param_t info);
// Set the flags for this tensor symbol. The flags are only used for symbol, not for tensor.
int ccv_nnc_tensor_symbol_set_flags(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor, const int flags);
// Get all the flags for a tensor
CCV_WARN_UNUSED(int) ccv_nnc_tensor_symbol_flags(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t tensor);
// Manually concatenate input graph nodes with an output graph node to create a new graph.
// Return non-zero if cannot concat successfully.
int ccv_nnc_graph_exec_symbol_concat(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t source, const ccv_nnc_graph_exec_symbol_t destination);
// Manually disconnect input graph nodes with an output graph node for this graph.
// Return non-zero if cannot disjoin successfully.
int ccv_nnc_graph_exec_symbol_disjoin(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t source, const ccv_nnc_graph_exec_symbol_t destination);
// Manually delete a exec symbol off the symbolic graph.
// Return non-zero if cannot free.
int ccv_nnc_graph_exec_symbol_free(ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_graph_exec_symbol_t symbol);
// Automatic concatenate these nodes together based on its inputs / outputs.
// Return non-zero if cannot figure out.
// Imagining this is to generate the execution flow based on input tensors and output tensors.
// nil for execs and 0 for exec_size means to loop over all the execs on the graph and autogen.
enum {
	CCV_NNC_AUTOGEN_ALL_EXECS = 0x1,
	CCV_NNC_AUTOGEN_SOURCES_AND_DESTINATIONS = 0x2,
};
int ccv_nnc_graph_exec_symbol_autogen(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const execs, const int exec_size, const int flags);
// Generate a duplicate of the provided graph.
// While generating the duplicate, it calls the function pointer to re-process the node type.
typedef ccv_nnc_cmd_t(*ccv_nnc_symbolic_graph_subst_f)(const ccv_nnc_graph_exec_symbol_t symbol, const ccv_nnc_cmd_t cmd);
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_dup(const ccv_nnc_symbolic_graph_t* const graph, ccv_nnc_symbolic_graph_subst_f subst);
// The source / destination generated by the autogen.
void ccv_nnc_symbolic_graph_set_sources(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size);
ccv_nnc_graph_exec_symbol_t* ccv_nnc_symbolic_graph_sources(const ccv_nnc_symbolic_graph_t* const graph);
int ccv_nnc_symbolic_graph_source_size(const ccv_nnc_symbolic_graph_t* const graph);
void ccv_nnc_symbolic_graph_set_destinations(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);
ccv_nnc_graph_exec_symbol_t* ccv_nnc_symbolic_graph_destinations(const ccv_nnc_symbolic_graph_t* const graph);
int ccv_nnc_symbolic_graph_destination_size(const ccv_nnc_symbolic_graph_t* const graph);
// Generate output that can be parsed by GraphViz (DOT language).
void ccv_nnc_symbolic_graph_dot(const ccv_nnc_symbolic_graph_t* const graph, const int flags, FILE* out);

typedef struct {
	ccv_nnc_tensor_symbol_t symbol;
	const ccv_nnc_tensor_t* tensor;
} ccv_nnc_tensor_bind_t;

// Compile a symbolic graph into a graph that can be executed, and a set of tensors (opaque data structure tensor arena) are allocated based on which tensor symbols are the input and which are the outputs. The tensor allocation is done to minimize the required storage.
// tensor_binds provide custom binding for these tensors. You still responsible to manage the life-time of these tensors.
// outputs marks the tensor symbols that need to be kept til the end of the graph.
void ccv_nnc_symbolic_graph_compile(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_bind_t* const tensor_binds, const int tensor_binds_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size, ccv_nnc_graph_t** const graph_ref, ccv_nnc_tensor_arena_t** const tensor_arena_ref, ccv_nnc_graph_exec_arena_t** const graph_exec_arena_ref);
// Free the symbolic graph and its associated memory. Note that if you compiled a graph / tensor arena out of this symbolic graph, these won't be free'd.
void ccv_nnc_symbolic_graph_free(ccv_nnc_symbolic_graph_t* const graph);
// Find corresponding tensor by a symbol from the tensor arena.
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_from_symbol(const ccv_nnc_tensor_arena_t* const tensor_arena, const ccv_nnc_tensor_symbol_t symbol);
// Bind a tensor to a symbol. You still responsible to manage the life-time of the tensor to make sure it is not freed until everything is done.
int ccv_nnc_tensor_bind_symbol(const ccv_nnc_tensor_arena_t* const tensor_arena, const ccv_nnc_tensor_symbol_t symbol, const ccv_nnc_tensor_t* const tensor);
// Free the opaque tensor arena structure.
void ccv_nnc_tensor_arena_free(ccv_nnc_tensor_arena_t* const tensor_arena);
// Find corresponding graph exec by a exec symbol from graph exec arena.
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_from_symbol(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena, const ccv_nnc_graph_exec_symbol_t symbol);
// Return the node that can drive all the source nodes from the compilation.
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_source(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena);
// Return the node that can drain all the destination nodes from the compilation.
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_exec_destination(const ccv_nnc_graph_exec_arena_t* const graph_exec_arena);
// Free the opaque graph exec arena structure.
void ccv_nnc_graph_exec_arena_free(ccv_nnc_graph_exec_arena_t* const graph_exec_arena);

/**
 * Level-3.5 API
 */

// Compute the backward graph, assuming the provided symbolic graph only contain the "forward" part from sources to destinations.
// This effectively is called the "autograd" or automatic differentiation process (specifically, "reverse AD") in other libs.
void ccv_nnc_symbolic_graph_backward(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size, const ccv_nnc_tensor_symbol_t* const f_symbols, const int f_symbol_size, const ccv_nnc_tensor_symbol_t* const wrt_symbols, const int wrt_symbol_size);
// Get the symbol that contains the gradient. The list will be flushed if the ccv_nnc_symbolic_graph_backward function is called again.
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_for_backward(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t symbol);
// This has to get the exec symbol from the tensor.
CCV_WARN_UNUSED(ccv_nnc_graph_exec_symbol_t) ccv_nnc_graph_exec_symbol_for_backward(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_tensor_symbol_t symbol);

// Construct a "while" loop in a symbolic graph.
//
// In NNC, a computation graph cannot allow cycles. Thus, there is no flexible way to express loops.
//
// A little survey on this problem:
//
// Caffe2 supports specific type of recurrent neural network.
// TensorFlow as it stands, supports while construct. Its while construct is very straightforward, a body and a condition is provided, you can construct whatever graph as you want.
// MxNet supports recurrent neural network by unrolling it into normal none-looped graph.
// Theano supports "scan" ops, which is a terminable loop (with loop variant, known as sequence).
// CNTK supports this with custom BrainScript. Within BrainScript, you can access the previous state in a function, therefore, effectively supports calling a method multiple times (looping over).
//
// Of above, Caffe2 and MxNet gave up on supporting generic loop for performance reasons.
// TensorFlow supports generic while loop, with all the trouble it may introduce (see the Nested while loop bug in TensorFlow that recently fixed).
// Theano picked a point seems pretty sweet, although there are limitations.
// CNTK's BrainScript is a DSL, they can do whatever they want with the drawback now that they need to implement a language runtime.
// TensorFlow, Theano and CNTK all support auto-differentiation over the while loop with tape (Wengert list).
//
// A simple way to support loop is to support conditional jump. In fact, conditional jump is a more generic way of doing loops. However,
// if you put this into the consideration that fully differentiable computation graph wanna to be supported, it is terrible. With conditional
// jump, it is really hard for you to know which tensor is used where, thus keep track for reverse accumulation (backward propagation). There
// is no counter or whatsoever, it is pretty hard to trace back on which line is executed how many times. Compounding this with NNC's promise
// that as long as it shows on the graph can be "parallel" computed, it will be parallel computed, it is close to impossible to track if
// conditional jump used in its raw form. Certain restrictions must be applied to how to do the loop. The compromise comes from closer
// examination of NNC's preferences.
//
// NNC prefers to have the graph without cycles. It also prefers to be fully differentiable. Another important criteria is that most
// functions in NNC require SSA (Static Single Assignment) representation. With these in mind, supporting while loop has to be strict.
//
// Luckily, there are well-formalized way of supporting this in literature and practice. Because it is well-formalized, translating this
// into existing NNC implementation is actually pretty straightforward. We are going to introduce a special version of while loop. In
// literature that discussed about SSA, it may be called parameterized loop. For us, it works like this:
//
// To construct a while loop for existing NNC graph, you need to be able to separate the existing graph into two sub-graphs.
//
// The while-loop sub-graph (WL sub-graph) contains a set of incoming nodes (I-nodes), Condition false output nodes (CFO-nodes) and end
// nodes (E-nodes). Each set have its own properties, but in short, all incoming edges to the WL sub-graph connect to one of the I-nodes,
// but nothing else. All outgoing edges from the WL sub-graph connect to one of the CFO-nodes, but nothing else. A nodes can be either a
// I-node, CFO-node or E-node, non-exclusively.
//
// There are also 3 types of tensors used for all nodes in WL sub-graph: Input tensors (I-tensors) are tensors that are inputs to some
// nodes, and will never be outputs. Output tensors (O-tensors) are tensors that are outputs from some nodes, but never be inputs to any
// nodes. I-tensors can be outputs from some nodes that outside of WL sub-graph. O-tensors can be inputs to some nodes that outside of
// WL sub-graph. Internal tensors (IN-tensors) are not visible outside of WL sub-graph, therefore, they can be both inputs and outputs of
// some nodes inside the sub-graph. Some tensors can be feedback into the WL sub-graph, given either O-tensors or IN-tensors. A parameter
// map can be given in these cases to describe which maps to what.
//
// The way to drive a WL sub-graph like this: the WL sub-graph runs until all CFO-nodes are reached. At this point, the while_f condition
// is checked. If true, we continue until all the end-nodes are reached. At this point, we increase the counter, reconfigure the WL
// sub-graph with parameter map, and run from I-nodes all over again. When reached all CFO-nodes, the condition is checked again, if false,
// WL sub-graph terminates, and the graph continues from the nodes that are pointed by CFO-nodes.
//
// Given these constraints, doing automatic differentiation is not that hard any more. A WL sub-graph, from the whole graph's point of view,
// is just a giant command supports both forward / backward operations, with some extra information passed around in the form of userdata
// (tape).
//
// For WL sub-graph, we can continue to leverage the compile / backward function that already written for symbolic graph as well.
//
// For compile function, we just need to take care of parameter maps (these need to be converted into binded tensors).
//
// For backward function, we need to convert parameter maps from assigner (thus, y = x) to accumulator (x += y).
//
// This function will replace the nodes that it affects to one sub-graph node.
// Thus, how to drive this sub-graph is opaque. Its backward form is opaque as well.
//
// There are no connection between its nodes and the outside graph nodes other than the three sets:
//
// 1). Incoming nodes, the set of nodes that contains the incoming edges from outside, they cannot have edges points by inside nodes.
//     The sub-graph computation starts from these incoming nodes;
// 2). Condition false output nodes, when condition is false, we will break out of this while loop, these nodes pointing to the outside
//     nodes, but no inside nodes;
// 3). End nodes, the set of nodes that marks the end of the while body, and after these nodes are executed, we will return to the
//     incoming nodes. These end nodes shouldn't have any edges pointing to inside nodes (OK if end nodes are condition true output nodes
//     as well);
//
// Since these will become a sub-graph (which, to its owner graph, just simple "node"), it will have inputs and outputs. Besides that,
// the loop body needs to be parameterized to be SSA compliant (see: https://www.cs.cmu.edu/~fp/courses/15411-f13/lectures/06-ssa.pdf).
// Thus, a list of body parameters need to be provided.

// The given tensors contains all the common / input / output tensors specified in the sub-graph.
// Currently, the special_size should always be 1, and contains only the loop counter.
typedef int(*ccv_nnc_graph_while_f)(ccv_nnc_tensor_t* const* const inputs, const int input_size, const void* const data);
// Opaque pointer to the tape of tensors. The tape are used by the while loop.
typedef struct ccv_nnc_tensor_tape_s ccv_nnc_tensor_tape_t;
CCV_WARN_UNUSED(ccv_nnc_tensor_tape_t*) ccv_nnc_tensor_tape_new(void);
void ccv_nnc_tensor_tape_io(ccv_nnc_tensor_tape_t* const tape, const ccv_nnc_graph_t* const graph, const int* const input_flags, ccv_nnc_tensor_t* const* const inputs, const int input_size, const int* const output_flags, ccv_nnc_tensor_t* const* const outputs, const int output_size);
uint64_t ccv_nnc_tensor_tape_numbering(ccv_nnc_tensor_tape_t* const tape, const ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec);
void ccv_nnc_tensor_tape_set_numbering(ccv_nnc_tensor_tape_t* const tape, ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, const uint64_t numbering);
void ccv_nnc_tensor_tape_free(ccv_nnc_tensor_tape_t* const tape);
// Augmented function to run a graph with while loop (An obvious example is dynamic RNN).
typedef struct ccv_nnc_tensor_multiview_s {
	// This is an augmented ccv_nnc_tensor_view_t
	// Namely, it can point to multiple versions of tensors.
	int type; // This type is CCV_NNC_TENSOR_MULTI_VIEW
	// kind specified how the multi-version tensors stored.
	// See the comment on the follow up enums.
	uint8_t kind;
	uint16_t repeat;
	intptr_t anchor; // on which graph this multi-view tensor is wrapped. This helps to determine on which level the multi-view tensor should be unwrapped.
	// If this tensor points to a tensor view, data.u8 - offset is the real pointer start.
	off_t offset;
	struct ccv_nnc_tensor_multiview_s* p; // If this is wrapped with another multiview tensor. Get to the parent one.
	ccv_nnc_tensor_t* it; // Current tensor (tensor in use), this is updated along with the graph computation.
	// This is useful because by just traverse tv, I can get the latest up-to-date reference to this multi-view tensor.
	ccv_array_t* sp; // Synchronized tensor views. This corresponds to ccv_nnc_tensor_synchronize_to_multiview method, that records all the tensors registered for updates.
	ccv_nnc_tensor_t* _inline_data[4];
	ccv_nnc_tensor_t** _heap_data;
} ccv_nnc_tensor_multiview_t;
#define CCV_NNC_MULTIVIEW_DATA(x) ((x)->_heap_data ? (x)->_heap_data : (x)->_inline_data)
#define CCV_NNC_MULTIVIEW_PHI (intptr_t)0x1 // Denote this is a phi multi-view tensor.

enum {
	CCV_NNC_MULTIVIEW_K0N = 0, // All of them are repeated.
	CCV_NNC_MULTIVIEW_K1N = 1, // The first one is the first, the second one starts to repeat. (0111111...)
};
#define CCV_NNC_MULTIVIEW_K01(x) ((x)->kind == CCV_NNC_MULTIVIEW_K0N && (x)->repeat == 1)
// Setup a tensor multiview with a given set of tensors.
void ccv_nnc_tensor_multiview(ccv_nnc_tensor_t* data[], const uint8_t kind, const uint16_t repeat, const ccv_nnc_graph_t* const graph, ccv_nnc_tensor_multiview_t* const tensor_multiview);
// Since tensor_multiview will never be allocated with *_new method, the *_free method simply frees anything that is dynamically allocated afterwards (such as the reference items).
void ccv_nnc_tensor_multiview_free(const ccv_nnc_tensor_multiview_t tensor_multiview);
// Setup a tensor as a reference to a tensor multiview, thus, when tensor multiview's tu (current tensor) updates, the tensor reference's data.u8 will get update as well (point to the same memory region as the tu).
void ccv_nnc_tensor_synchronize_to_multiview(ccv_nnc_tensor_multiview_t* const tensor_multiview, ccv_nnc_tensor_t* const tensor);
// Send broadcast to subscribers of the multiview, call this in the beginning of exec.
void ccv_nnc_tensor_multiview_synchronize(ccv_nnc_tensor_multiview_t* const tensor_multiview);
// Constructing looped concrete graph. Note that this interface is a little bit simpler than the one for symbolic graph. The reason
// is that a concrete graph operates on allocated tensors, thus, there is no mapping of tensor symbols between the parent graph
// and the while graph. (The reason to have a mapping in symbolic graphs is to constraint the variable leaking between the sub graph
// and parent graph).
CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_while(ccv_nnc_graph_t* const graph, const uint32_t cmd, ccv_nnc_graph_t* const while_graph);
CCV_WARN_UNUSED(ccv_nnc_graph_t*) ccv_nnc_graph_from_graph_exec(const ccv_nnc_graph_t* const graph, ccv_nnc_graph_exec_t exec);
void ccv_nnc_graph_set_while_expr(ccv_nnc_graph_t* const while_graph, const ccv_nnc_graph_while_f while_expr, const void* const while_data, ccv_nnc_tensor_t* const* const inputs, const int input_size, const ccv_nnc_graph_exec_t* const breakpoints, const int breakpoint_size);
CCV_WARN_UNUSED(ccv_nnc_tensor_t) ccv_nnc_tensor_for_while_count(const ccv_nnc_graph_t* const while_graph);
// In that case, the computation graph still has no loops or cycles, but you can run it multiple times against different
// versions of the tensors until the condition not met (thus, the tensor is versioned, so you can "backpropagate through time").
int ccv_nnc_graph_run(ccv_nnc_graph_t* const graph, ccv_nnc_tensor_tape_t* const tensor_tape, const int flags, const ccv_nnc_graph_exec_t* const sources, const int source_size, const ccv_nnc_graph_exec_t* const destinations, const int destination_size);

// The API to operate on the symbolic graph is more involved than the concrete graph for while loops.
// The reason is because symbolic graph operates in SSA form (static single assignment), therefore, the while loops
// for the symbolic graph has to be parameterized.

// Return a while exec symbol (backed by a sub-graph) of the giving graph. The exec nodes on the way from sources to destinations
// will be moved from the giving graph to the sub-graph.
ccv_nnc_graph_exec_symbol_t ccv_nnc_symbolic_graph_while(ccv_nnc_symbolic_graph_t* const graph, const uint32_t cmd, ccv_nnc_symbolic_graph_t* const while_graph, const char* const name);
// Set the expression to be evaluated, and at which nodes to be evaluated.
void ccv_nnc_symbolic_graph_set_while_expr(ccv_nnc_symbolic_graph_t* const while_graph, const ccv_nnc_graph_while_f while_expr, const void* const while_data, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_graph_exec_symbol_t* const breakpoints, const int breakpoint_size);
// Set the loop carry parameters when reuse. (parameterized loop, these will be carried over to the next loop).
void ccv_nnc_symbolic_graph_set_carry_overs(ccv_nnc_symbolic_graph_t* const while_graph, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);
// Retrieve the special (magical) tensor symbol that retains the while loop counter (thus, dimension of 1x1x1, CCV_64S type).
CCV_WARN_UNUSED(ccv_nnc_tensor_symbol_t) ccv_nnc_tensor_symbol_for_while_count(const ccv_nnc_symbolic_graph_t* const while_graph);
// Extract the sub-graph of the while loop from a symbol.
CCV_WARN_UNUSED(ccv_nnc_symbolic_graph_t*) ccv_nnc_symbolic_graph_from_while_symbol(const ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t while_symbol);

// Construct "switch" control structure in symbolic graph. Here I use the keyword case_of.
//
// Providing a "switch" control structure within NNC has some nice properties even though you can simulate this with a while loop technically.
//
// 1. More optimal memory allocation: with "switch" control structure, memory can be multiplexed for each code path because they are mutually
//    exclusive.
// 2. No tape should be used within each branch: if we simulate with a "while" loop, any results from within the "switch" statement has to be
//    kept on the tape, which is inefficient because you don't need any tape for the "switch" statement other than record which path it is
//    taken.
//
// The particular "switch" control structure provided here is a multi-way structured "switch". Each branch is a sub-graph, so it is well-scoped.
// A node branch out based on the case_of condition return value to either of the branch (numbering from 0 to n, -1 means no path taken). If no
// path taken, the output tensors will be assigned with the default tensors and continue. Otherwise the computation within the sub-graph will be
// carried out and the output tensors will be assigned with the tensors specified within that sub-graph and continue.
//
// If we want to consider speculative execution in the future, we need to revisit our memory allocation scheme.

typedef int(*ccv_nnc_graph_case_of_f)(ccv_nnc_tensor_t* const* const inputs, const int input_size, const void* const data);
CCV_WARN_UNUSED(ccv_nnc_graph_exec_symbol_t) ccv_nnc_symbolic_graph_case_of_new(ccv_nnc_symbolic_graph_t* const graph, const uint32_t cmd, const ccv_nnc_tensor_symbol_t* const inputs, const int input_size, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size, const char* const name);
void ccv_nnc_symbolic_graph_set_case_of_expr(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t exec, ccv_nnc_graph_case_of_f case_of, const void* case_of_data);
void ccv_nnc_symbolic_graph_set_case_of(ccv_nnc_symbolic_graph_t* const graph, const ccv_nnc_graph_exec_symbol_t symbol, ccv_nnc_symbolic_graph_t* const case_graph, const int case_of, const ccv_nnc_tensor_symbol_map_t* const symbol_map, const int symbol_map_size);

CCV_WARN_UNUSED(ccv_nnc_graph_exec_t) ccv_nnc_graph_case_of_new(ccv_nnc_graph_t* const graph, const uint32_t cmd, ccv_nnc_tensor_t* const* const inputs, const int input_size, ccv_nnc_tensor_t* const* const outputs, const int output_size, const int argument_offset, const int argument_size);
void ccv_nnc_graph_set_case_of_expr(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_graph_case_of_f case_of, const void* case_of_data, const int offset);
void ccv_nnc_graph_set_case_of(ccv_nnc_graph_t* const graph, const ccv_nnc_graph_exec_t exec, ccv_nnc_graph_t* const case_graph, const int case_of);

// Symbolic graph simplification.
//
// We make a distinction between graph simplifications and optimizations (autotune).
//
// Simplification: rewrite the graph and the resulting graph will have less nodes. This is done on the symbolic graph only. Passes that is
// "simplification" include pruning, common sub-expression eliminations, constant folding etc.
//
// Optimization (autotune): graph optimization can have more objectives. The most obvious objective is to reduce computation time. For
// symbolic graph, passes that reduces computation time include data layout optimizations, auto parallel etc (in normal optimization
// implementations, they have a cost model to guide the optimization. NNC's implementation uses a cost database that profiles the time
// cost on the device to guide the optimization. We call it autotune to distinguish with the normal optimization passes because we need
// device profile data). There could be other objectives, for example, in many deep learning applications, reducing memory footprint can
// be desirable. However, as always in computer science, memory and time is a typical trade-off. Memory optimization almost always results
// longer computation time, and the objective is to trade between these two with a bias term (in other frameworks such as TensorFlow, the
// memory optimizer uses a list of "cheap ops" to bias between the time and memory footprint).
//
// For graph optimizations, it can happen on both the symbolic graph level as well as the concrete graph level. For NNC, symbolic graph
// is already very explicit (data layout, device allocation and data transfer between devices / nodes, even the command backend can all
// be specified on the symbolic graph), however, some information is unknown until it is compiled down to concrete graph (tensor addresses,
// tensor initialization etc.), and since graph optimizations need all the information to optimize. Keeping the flexibility to do
// optimization on both symbolic and concrete graph level seems reasonable.

enum {
	// If two commands generated the same outputs, all the places where the newer output used will be replaced by the old output.
	// Later on the graph pruning stage, the command that generate the newer output will be eliminated.
	CCV_NNC_SIMPLIFY_COMMON_SUBEXPRESSION_ELIMINATION,
	// For the given outputs, eliminate unused input tensors, and then eliminate graph execs that don't contribute to the outputs.
	CCV_NNC_SIMPLIFY_GRAPH_PRUNING,
	// CCV_NNC_SIMPLIFY_CONSTANT_FOLDING, // This currently is not supported, because we don't have efficient way to express constant in symbolic graph.
};
// When a graph is simplified, its sources / destinations are changed as well.
void ccv_nnc_symbolic_graph_simplify(ccv_nnc_symbolic_graph_t* const graph, const int* const passes, const int pass_size, const ccv_nnc_tensor_symbol_t* const outputs, const int output_size, const ccv_nnc_graph_exec_symbol_t* const sources, const int source_size, const ccv_nnc_graph_exec_symbol_t* const destinations, const int destination_size);

/**
 * Level-4 API
 */

// Opaque pointer to the dynamic graph structure.
typedef struct ccv_nnc_dynamic_graph_s ccv_nnc_dynamic_graph_t;

// Masquerade this as if it is a on stack variable, there is a heap allocation but managed by the dynamic graph.
// The fact that ccv_nnc_tensor_variable_t is a pointer is an implementation detail. It should be treated as an
// opaque type throughout. We may later extends this to be some on-stack information or even just a uid.
typedef struct ccv_nnc_tensor_variable_s* ccv_nnc_tensor_variable_t;

// Create a dynamic graph.
CCV_WARN_UNUSED(ccv_nnc_dynamic_graph_t*) ccv_nnc_dynamic_graph_new(void);
// Get a new tensor variable.
CCV_WARN_UNUSED(ccv_nnc_tensor_variable_t) ccv_nnc_tensor_variable_new_impl(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_param_t info);
#define CCV_NNC_TENSOR_VARIABLE_NEW_X_1(graph) ccv_nnc_tensor_variable_new_impl(graph, ccv_nnc_tensor_auto)
#define CCV_NNC_TENSOR_VARIABLE_NEW_X_SEL(_1, _2, _FX, ...) _FX
// Making so that this new method can take parameters for both no parameter or with tensor_param.
#define ccv_nnc_tensor_variable_new(graph, ...) CCV_NNC_TENSOR_VARIABLE_NEW_X_SEL(graph, ##__VA_ARGS__, ccv_nnc_tensor_variable_new_impl, CCV_NNC_TENSOR_VARIABLE_NEW_X_1)(graph, ##__VA_ARGS__)
// Create a new tensor variable that is an alias of a given tensor variable.
CCV_WARN_UNUSED(ccv_nnc_tensor_variable_t) ccv_nnc_tensor_variable_alias_new(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable, const int ofs[CCV_NNC_MAX_DIM_ALLOC], const int inc[CCV_NNC_MAX_DIM_ALLOC], const ccv_nnc_tensor_param_t info);
// Get the underlying tensor for the tensor variable. The tensor allocation may be performed when calling this method.
CCV_WARN_UNUSED(ccv_nnc_tensor_t*) ccv_nnc_tensor_from_variable(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable);
// Execute a command with given tensor variables, the output is in the output tensor variables.
int ccv_nnc_dynamic_graph_exec(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_cmd_t cmd, const ccv_nnc_hint_t hint, const int flags, const ccv_nnc_tensor_variable_t* const inputs, const int input_size, ccv_nnc_tensor_variable_t* const outputs, const int output_size);
// Compute the gradient of given tensor, with respect to the f. Thus, df / dt.
void ccv_nnc_dynamic_graph_backward(ccv_nnc_dynamic_graph_t* const dynamic_graph, const ccv_nnc_tensor_variable_t f_variable, const ccv_nnc_tensor_variable_t* const inputs, const int input_size, ccv_nnc_tensor_variable_t* const outputs, const int output_size);
// Dispose a tensor variable. You cannot do any computation against this tensor variable afterwards.
void ccv_nnc_tensor_variable_free(ccv_nnc_dynamic_graph_t* const graph, const ccv_nnc_tensor_variable_t tensor_variable);
// Free the dynamic graph.
void ccv_nnc_dynamic_graph_free(ccv_nnc_dynamic_graph_t* const graph);
// Generate output that can be parsed by GraphViz (DOT language).
void ccv_nnc_dynamic_graph_dot(const ccv_nnc_dynamic_graph_t* const graph, const int flags, FILE* out);

/**
 * Level-5 API
 */

// Dataframe
//
// I am still flushing out the details of this implementation. Dataframe here is very similar to pandas dataframe
// but instead of operating scalars (mostly), it operates tensors.
//
typedef struct ccv_cnnp_dataframe_s ccv_cnnp_dataframe_t;
CCV_WARN_UNUSED(ccv_cnnp_dataframe_t*) ccv_cnnp_dataframe_new(void);
void ccv_cnnp_dataframe_free(ccv_cnnp_dataframe_t* const dataframe);

// Model
//
// With Keras API in mind, this model implementation essentially is a light-weight way to group neural network layers
// together.
//
typedef struct ccv_cnnp_model_s {
} ccv_cnnp_model_t;
CCV_WARN_UNUSED(ccv_cnnp_model_t*) ccv_cnnp_sequential_new(ccv_cnnp_model_t* const* const models, const int model_size);
void ccv_cnnp_model_free(ccv_cnnp_model_t* const model);

#endif
